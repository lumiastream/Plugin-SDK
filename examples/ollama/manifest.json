{
	"id": "ollama",
	"name": "Ollama",
	"version": "1.0.2",
	"author": "Lumia Stream",
	"email": "dev@lumiastream.com",
	"website": "https://lumiastream.com",
	"description": "Send prompts to a local Ollama server and use responses in Lumia templates via {{ollama_prompt}} and related helpers.",
	"license": "MIT",
	"lumiaVersion": "^9.0.0",
	"category": "apps",
	"keywords": "ollama, ai, chat, llm, local",
	"icon": "ollama.png",
	"config": {
		"hasAI": true,
		"settings_tutorial": "./settings_tutorial.md",
		"translations": "./translations.json",
		"settings": [
			{
				"key": "baseUrl",
				"label": "Base URL",
				"type": "text",
				"defaultValue": "http://localhost:11434",
				"required": true,
				"helperText": "Your Ollama server URL.",
				"refreshOnChange": true,
				"section": "Connection",
				"sectionOrder": 1,
				"group": "connection"
			},
			{
				"key": "defaultModel",
				"label": "Default Model",
				"type": "select",
				"allowTyping": true,
				"dynamicOptions": true,
				"refreshOnChange": true,
				"placeholder": "gpt-oss:20b",
				"options": [
					{
						"label": "Auto (first available)",
						"value": ""
					}
				],
				"required": false,
				"helperText": "Loaded from Ollama /api/tags. Leave blank for auto-detect or type a custom model.",
				"section": "Connection",
				"sectionOrder": 1,
				"group": "connection"
			},
			{
				"key": "defaultSystemMessage",
				"label": "Default System Message",
				"type": "textarea",
				"rows": 3,
				"helperText": "Optional system message used when none is provided in the action.",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": {
					"key": "advanced_tuning",
					"label": "Advanced Tuning",
					"helperText": "Optional controls for sampling, memory, and output behavior."
				}
			},
			{
				"key": "defaultTemperature",
				"label": "Default Temperature",
				"type": "number",
				"min": 0,
				"max": 2,
				"step": 0.1,
				"helperText": "Optional. Higher is more creative.",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": "advanced_tuning"
			},
			{
				"key": "defaultTopP",
				"label": "Default Top P",
				"type": "number",
				"min": 0,
				"max": 1,
				"step": 0.05,
				"helperText": "Optional nucleus sampling value.",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": "advanced_tuning"
			},
			{
				"key": "defaultMaxTokens",
				"label": "Default Max Tokens",
				"type": "number",
				"min": 1,
				"max": 8192,
				"helperText": "Optional. Maps to Ollama num_predict.",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": "advanced_tuning"
			},
			{
				"key": "keepAlive",
				"label": "Keep Alive",
				"type": "text",
				"placeholder": "5m",
				"helperText": "How long to keep the model loaded (example: `5m`, `0`). Optional.",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": "advanced_tuning"
			},
			{
				"key": "requestTimeoutMs",
				"label": "Request Timeout (ms)",
				"type": "number",
				"defaultValue": 0,
				"min": 0,
				"max": 300000,
				"helperText": "How long to wait for a response. Set to 0 to disable timeout.",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": "advanced_tuning"
			},
			{
				"key": "rememberMessages",
				"label": "Remember Messages",
				"type": "toggle",
				"defaultValue": true,
				"helperText": "Store history per thread or username.",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": "advanced_tuning"
			},
			{
				"key": "maxHistoryMessages",
				"label": "Max History Messages",
				"type": "number",
				"defaultValue": 12,
				"min": 0,
				"max": 100,
				"helperText": "How many recent messages to keep per thread/user.",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": "advanced_tuning"
			},
			{
				"key": "maxOutputChars",
				"label": "Max Output Length (chars)",
				"type": "number",
				"defaultValue": 0,
				"min": 0,
				"max": 100000,
				"helperText": "Trim responses to this length (0 = no limit).",
				"section": "Advanced",
				"sectionOrder": 2,
				"group": "advanced_tuning"
			}
		],
		"actions": [],
		"variableFunctions": [
			{
				"key": "ollama_prompt",
				"label": "Ollama Prompt",
				"description": "Use {{ollama_prompt=message|thread|model}} to return a response from Ollama."
			},
			{
				"key": "ollama_json",
				"label": "Ollama JSON",
				"description": "Use {{ollama_json=message|thread|model}} to return JSON-only output."
			},
			{
				"key": "ollama_one_line",
				"label": "Ollama One Line",
				"description": "Use {{ollama_one_line=message|thread|model}} to return a single-line response."
			},
			{
				"key": "ollama_prompt_nostore",
				"label": "Ollama Prompt (No Store)",
				"description": "Use {{ollama_prompt_nostore=message|thread|model}} to run without history."
			},
			{
				"key": "ollama_prompt_clear",
				"label": "Ollama Clear Thread",
				"description": "Use {{ollama_prompt_clear=thread_name}} to clear a conversation thread."
			}
		],
		"variables": [],
		"alerts": []
	}
}
